---
title: 数学基础——基于pytorch
date: 2023-12-15
---

# 数据操作

###### 创建向量

序列创建

```python
x = torch.arange(12) 		# tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```

访问张量（沿每个轴的长度）的*形状* 

```python
x.shape						# torch.Size([12])
```

张量中元素的总数

```python
x.numel()					# 12
```

改变一个张量的形状而不改变元素数量和元素值

```python
X = x.reshape(3, 4)			#tensor([[ 0,  1,  2,  3],
        					#		 [ 4,  5,  6,  7],
        					#		 [ 8,  9, 10, 11]])
```

全0、全1，随机，确定值

```
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))
torch.randn(3, 4)
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
```

###### 运算符

```python
x + y, x - y, x * y, x / y, x ** y  					# 加减乘除幂
torch.exp(x)											# 求幂
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)		# 按dim轴连结
X == Y													# 逻辑
X.sum()													# 求和
```

###### 广播

广播机制：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

```python
a = torch.arange(3).reshape((3, 1))						#(tensor([[0],
         												#		  [1],
         												#		  [2]]),
        
b = torch.arange(2).reshape((1, 2))						#tensor([[0, 1]]))

# a,b形状不匹配，相加会触发广播机制
# 矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。
a + b													# array([[0., 1.],
       													#		 [1., 2.],
       													#		 [2., 3.]])

```

###### 索引和切片

```python
pythonX[-1], X[1:3]		# 索引从0开始
X[1, 2] = 9				# 赋值
X[0:2, :] = 12			# 轴1（列）赋值
```

###### 节省内存

在Python中，某些操作会导致创建新的对象，而不是修改现有对象。

这可能是不可取的，原因有两个：

1. 不必要的内存分配会浪费资源，尤其是在频繁更新大数据集（ML通常有数百M数据集）时；
2. 如果不是原地更新，其他引用还会指向旧数据，可能导致代码错误地使用过时信息。

```python
#浪费内存用法
y = y + x  		# 这会计算y + x的结果，并将其赋值给一个新的对象，然后让变量y引用这个新对象。

#节省内存用法
y += x     		# 这个就地操作会直接在y所引用的对象上累加x的值，避免了创建新对象，因此更节省内存。
y[:] = x + y  	# 对于支持切片操作的对象，这个语法会在y所引用的对象上就地应用x + y的结果，也避免了新对象的创建。
```

###### 转换为其他Python对象

```
A = X.numpy()
B = torch.tensor(A)
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```

