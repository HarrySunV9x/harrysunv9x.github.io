# 定义

机器学习 (ML) 是[人工智能 (AI)](https://www.netapp.com/zh-hans/artificial-intelligence/what-is-artificial-intelligence) 的一部分，属于计算科学领域，专门分析和解释数据的模式及结构，以实现无需人工交互即可完成学习、推理和决策等行为的目的。简单来说，机器学习即支持用户向计算机算法馈送大量数据，然后让计算机分析这些数据，并仅根据输入数据给出数据驱动型建议和决策。如果算法识别出任何更正，它会整合更正信息，改进未来决策。

来源：https://www.netapp.com/zh-hans/artificial-intelligence/what-is-machine-learning/

# 分类

1. **监督学习（Supervised Learning）**：这是最常见的机器学习类型。在监督学习中，我们有输入变量（X）和输出变量（Y），并使用一个或多个算法来学习从输入映射到输出的映射函数。目的是使算法能够对新的输入数据做出预测。例如，用房屋的特征来预测房价。
2. **无监督学习（Unsupervised Learning）**：在这种学习中，只有输入数据（X）而没有相应的输出变量。无监督学习的目的是对数据的结构进行建模，以便更好地理解数据。例如，对顾客进行市场细分。
3. **半监督学习（Semi-supervised Learning）**：这种方法介于监督学习和无监督学习之间。它使用大量未标记的数据和少量标记的数据来训练模型。这种方法通常用于标记数据成本较高的情况。
4. **强化学习（Reinforcement Learning）**：在强化学习中，算法通过与环境的互动来学习做出决策。它不是被告知哪些行为是最好的，而是通过试错的过程发现哪些行为会获得最大的奖励。
5. **深度学习（Deep Learning）**：虽然深度学习通常被视为一种算法，但它也可以视为一种机器学习的方法，特别是它使用多层神经网络来学习数据。深度学习在图像识别、语音识别等领域表现突出。
6. **迁移学习（Transfer Learning）**：在这种方法中，模型在一个任务上训练，并在另一个相关任务上进行再训练或微调。这在数据较少的新任务上特别有用。

# 算法

- **监督学习(supervised learning)（预测）**

  - 定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值(称为回归），或是输出是有限个离散值（称作分类）。

  - In：有标签，Out：有反馈

    目的：预测结果

    案例：猫狗分类，房价预测

  - **分类 k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络**

  - **回归 线性回归、岭回归**

- **无监督学习(unsupervised learning)**

  - 定义：输入数据是由输入特征值所组成。

  - In：无标签，Out：无反馈

    目的：发现潜在结构

    案例：“物以类聚，人以群分”

  - **聚类 k-means**，**降维**

- **半监督学习**

  - 已知：训练样本Data和待分类的类别

    未知：训练样本有无标签均可

    应用（案例）：训练数据量过多时，监督学习效果不能满足需求，因此用来增强效果。

- **强化学习**

  - In：决策流程及激励系统，Out：一系列行动

    目的：长期利益最大化，回报函数（只会提示你是否在朝着目标方向前进的延迟反映）

    案例：学下棋

    **算法：马尔科夫决策，动态规划**

# 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。

常见的损失函数包括：

1. **均方误差（Mean Squared Error, MSE）**: 用于回归问题，计算预测值与实际值之差的平方的平均值。
   $$
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 
   $$
   其中，Yi是第 i 个样本的真实值，Y*^*i 是模型对第 i 个样本的预测值，n 是样本数量。

2. **交叉熵损失（Cross-Entropy Loss）**: 用于分类问题，尤其是在二分类和多分类问题中，衡量实际的输出分布和预测的输出分布之间的差异。

   - 二分类问题
     $$
     \text{Cross-Entropy} = -\sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
     $$
     其中，yi 是真实标签（0 或 1），y*^*i 是预测为正类的概率。

   - 多分类问题
     $$
     \text{Cross-Entropy} = -\sum_{i=1}^{n} \sum_{c=1}^{C} y_{ic} \log(\hat{y}_{ic})
     $$
     其中，C 是类别总数，yic 是一个指示器，如果样本 i 属于类 c 则为 1，否则为 0；y^ic 是模型预测样本 i属于类 c 的概率。

3. **绝对值误差（Mean Absolute Error, MAE）**: 也用于回归问题，计算预测值与实际值之差的绝对值的平均。
   $$
   \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y}_i|
   $$
   

4. **Hinge损失（Hinge Loss）**: 常用于支持向量机（SVM）中，特别适用于“最大间隔”分类。
   $$
   \text{Hinge Loss} = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)
   $$

5. **对数损失（Log Loss）**: 在一些分类问题中使用，特别是在概率估计方面。
   $$
   \text{Log Loss} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
   $$

# 梯度下降