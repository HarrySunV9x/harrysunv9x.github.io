---
title: 机器学习的完整流程——线性回归训练实例
date: 2024-02-10
---

# 线性回归训练实例

## 流程

刚读研那年，大数据很火。

我是一个很爱计算机的人，从河北二本电气考入上海985软工，实际上并不是图这个行业的高薪资，而是在我拿到iPhone4的那一刻，我是真的想做一名可以参与改变世界的工程中的一颗螺丝钉。对于火热的大数据，我自然不想错过这个方向。

没有人会要一个什么都不会的人，很幸运，最后被彭老师收留，并委任了我前端的工作，负责实验室的门面。前端的编码比较简单，很容易上手。JavaScript的性能很差，在数据流框架还没火起来时，jQuery还是霸主，前端的方向也很窄：没有后端脚本，没有工程化，没有打包工具。但是他有一个特点：他的语法和Python太像了。

如今从业近两年，ChatGPT大火，再一次点燃了我的技术热情，恰好，前端后端底层上层系统应用网络自己都稍微接触了一些，对编码有了更宏观的理解，是一个很好的时机去深入了解一下这门学科。

本文旨在通过线性回归的实例，完整的说明机器学习完整的过程。

本文内容来自：[《动手学深度学习》3.2. 线性回归的从零开始实现](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)

机器学习的流程包括：

1. **数据准备**：首先需要收集并准备训练数据。这个阶段包括数据清洗、处理缺失值、特征选择和转换等，以确保数据的质量和适用性。高质量的数据是建立有效模型的基础。
2. **数学建模**：选择或构建一个适合特定问题的数学模型。本例将使用线性回归模型：模型假设输出是输入特征的线性组合。根据应用场景的不同，也可以选择决策树、神经网络等其他类型的模型。
3. **损失函数**：定义一个损失函数来衡量模型预测值与实际值之间的差异。选择合适的损失函数对于模型训练效果至关重要，回归问题常用均方误差（MSE）。
4. **训练模型**：采用如梯度下降等优化算法调整模型参数，目的是最小化损失函数。通过计算损失函数的梯度来更新模型参数，从而提高模型的准确度。
5. **模型评估**：模型训练完成后，使用验证集或测试集对其性能进行评估。常用的评估指标包括准确度、召回率、精确度等，以确定模型的有效性和可靠性。
6. **超参数调优**：根据模型评估的结果，调整模型的超参数（例如学习率、正则化系数等），以进一步提高模型的性能和准确度。
7. **推理（预测）**：在模型经过训练、评估和调优之后，最后的步骤是使用该模型进行预测。这包括将新的输入数据输入到模型中，以获得预测结果。

## 数据准备

本文不对实际的数据进行训练，而是通过一个函数来生成一组线性数据，供我们后续训练使用。

线性回归的模型为：
$$
y = \mathbf{wX} + b
$$
是一个很简单的一次方程。但是需要注意的是，这里仅仅是一次，不一定是一元。这里的**X**是一个矩阵，可能包含多个特征值：
$$
y = \begin{bmatrix} 
x_1 & x_2 & \cdots & x_n 
\end{bmatrix} 
\begin{bmatrix} 
w_1 \\
w_2 \\
\vdots \\
w_n 
\end{bmatrix} + b
$$
也就是：
$$
y = w_1x_1 + w_2x_2 + w_3x_3 + \cdots + b
$$


对于确定的数据，w和b都是确定的，我们假设有一组确定的w, b：

```python
# 初始化真实的权重和偏置
true_w = torch.tensor([[2], [-3.4]])  # 真实的权重
true_b = 4.2  # 真实的偏置
```

根据上面的式子，**w**代表模型权重，数量要和**X**是一致，此处创建了一个**PyTorch张量（Tensor）**用来代表模型的权重**w**。我们可以将这个 PyTorch 张量理解成是一种特殊的数据结构，他和普通数组、以及 NumPy 数组都是不一样的。PyTorch 张量具有**GPU 支持**与**自动微分**的特性。这在机器学习的处理中十分有用。

在这个例子中，权重**w**是一个包含两个元素的列向量，分别为 `2` 和 `-3.4`。

而b显然是一个常数，我们这里设置成了4.2。

于是，我们这个用于生成数据的模型为：
$$
y=2x_1−3.4x_2+4.2
$$
然后，根据这两个参数实现生成数据的函数：

```python
def synthetic_data(w, b, num_examples):
    # 生成 `num_examples` 个正态分布的随机输入数据（特征）
    X = torch.normal(0, 1, (num_examples, len(w)))

    # 使用线性模型（y = wx + b）计算相应的输出数据（标签）
    y = torch.matmul(X, w) + b

    # 向标签中添加正态分布的噪声
    y += torch.normal(0, 0.01, y.shape)
    return X, y
  
# 生成合成特征和标签
features, labels = synthetic_data(true_w, true_b, 1000)
```

features 即 **X**，通常称之为特征； labels 为每一组特征对应的 y，通常称之为标签。

X是利用torch.normal随机生成的。

- `torch.normal(mean, std, size)`是 PyTorch 中用来生成正态（高斯）分布随机数的函数。

  - `mean` 是分布的均值；

  - `std` 是分布的标准差；

  - `size = (num_examples, len(w))`指定生成的张量的形状。`num_examples` 是要生成的样本数，`len(w)` 是每个样本的特征数量。

    normal的第三个参数穿的是一个Torch.Size()的参数，这实际上是一个元祖，我们可以用简单的示例验证一下：

    ```python
    import torch
    
    tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])
    shape = tensor.shape
    print(shape == (2,3)) 			# 打印 True
    ```

- `torch.matmul` 是 PyTorch 中的矩阵乘法函数，用于计算两个张量（Tensor）的乘积。

- 为了防止数据过于理想与纯净，最后增加一个符合正态分布的噪声，防止过拟合，提高模型的鲁棒性和泛化能力。

> 在原文中，w的初始值为：true_w = torch.tensor([2, -3.4])，这是一个横向量，和上面的式子是不一致的。
>
> 这也导致 y = torch.matmul(X, w) + b 的结果被压缩成了一维行向量，这不是我们应该得到的结果。
>
> 最终返回值也进行了变形 return X, y.reshape(-1, 1)
>
> 我个人认为这是不好的操作，所以我做了一些修改，使其更直观，更符合直觉。

最终，我们得到了包含一千个数据的X，y，以供后续训练使用

## 数据读取

很容易想到，我们不可能一次对完整的数据集进行训练，通常来说，我们训练模型时要对数据集进行遍历，每次抽取一小批量样本，并用他们更新模型。

接下来，我们通过实现一个函数，打乱数据集的样本并进行小批量的读取。

```python
# 数据迭代的批量大小
batch_size = 10

# 打乱数据集并批量读取数据
def data_iter(batch_size, features, labels):
    # 示例的数量
    num_examples = len(features)

    # 为示例创建索引
    indices = list(range(num_examples))

    # 打乱索引以随机化数据顺序
    random.shuffle(indices)

    # 以批量形式返回数据
    for i in range(0, num_examples, batch_size):
      	# 索引要使用张量索引
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])	
        # 切片索引
        # yield用于创建一个生成器（generator）。允许你暂停函数的执行。每次调用函数时都从暂停的地方继续。
        yield features[batch_indices], labels[batch_indices]												
```

## 数学建模

有了数据，就可以对需要的场景进行建模，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。 

本文的例子使用线性回归模型：

```python
# 线性回归模型
def liner_regression(X, w, b):
  	# torch.matmul(tensor1, tensor2), 用于矩阵乘法
    return torch.matmul(X, w) + b
```

有了数学模型，在开始寻找最好的*模型参数*（model parameters）w和b之前， 我们还需要两个东西：

（1）一种模型质量的度量方式——损失函数； 

（2）一种能够更新模型以提高模型预测质量的方法——随机梯度下降函数

## 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距，回归问题中最常用的损失函数是平方误差函数：
$$
L(y, \hat{y}) = \frac{1}{2} \sum (y - \hat{y})^2
$$
其中，y'为真实值，y为预测值。

```python
# 损失函数（均方误差）
def loss_function(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

## 随机梯度下降

*梯度下降*（gradient descent）的方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。这里直接套公式（具体原理暂时没看懂）：
$$
w \leftarrow w - \frac{\eta}{\lvert B \rvert} \sum_{i \in B} \nabla_w l^{(i)}(w, b) = w - \frac{\eta}{\lvert B \rvert} \sum_{i \in B} x^{(i)} \left( w^T x^{(i)} + b - y^{(i)} \right),
$$

$$
b \leftarrow b - \frac{\eta}{\lvert B \rvert} \sum_{i \in B} \nabla_b l^{(i)}(w, b) = b - \frac{\eta}{\lvert B \rvert} \sum_{i \in B} \left( w^T x^{(i)} + b - y^{(i)} \right).
$$

代码实现：

```python
# 随机梯度下降函数
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            # 使用其梯度更新参数
            param -= lr * param.grad / batch_size
            # 更新后将梯度归零
            param.grad.zero_()
```

## 训练

准备好各种函数后，就可以开始训练了：

```python
# 初始化模型参数
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# 学习率
lr = 0.03

# 训练周期的数量
num_epochs = 3

# 指定模型和损失函数
net = liner_regression
loss = loss_function

# 训练循环
for epoch in range(num_epochs):
    # 遍历数据集
    for X, y in data_iter(batch_size, features, labels):
        # 计算损失
        l = loss(net(X, w, b), y)
        # 计算梯度
        l.sum().backward()
        # 更新参数
        sgd([w, b], lr, batch_size)
    # 计算并打印每个周期后的损失
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

# 估量误差
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

## 运用库函数的简洁实现

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l
from torch import nn

# 设置用于合成数据的真实权重和偏置
true_w = torch.tensor([2, -3.4])
true_b = 4.2

# 使用真实的权重和偏置生成合成数据（特征和标签）
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

# 用于将数据集加载到PyTorch DataLoader的函数
def load_array(data_arrays, batch_size, is_train=True):
    """构建一个PyTorch数据迭代器。"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

# 设置批处理大小并创建数据迭代器
batch_size = 10
data_iter = load_array((features, labels), batch_size)

# 获取一批数据
next(iter(data_iter))

# 定义一个简单的神经网络，包含单个线性层
net = nn.Sequential(nn.Linear(2, 1))

# 使用正态分布和常数值初始化网络的权重和偏置
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

# 定义损失函数（均方误差损失）
loss = nn.MSELoss()

# 设置优化器（随机梯度下降），学习率为0.03
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

# 训练模型的轮数
num_epochs = 3

# 训练循环
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()  # 清除之前的梯度
        l.backward()         # 计算梯度
        trainer.step()       # 更新参数
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

# 检索并打印训练后估计的权重和偏置误差
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)

```

